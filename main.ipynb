{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from models import *\n",
    "from configs import cfg\n",
    "import pandas as pd\n",
    "from nltk.translate import bleu_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    # From the csv file given by filename and return a pandas DataFrame of the read csv.\n",
    "    \n",
    "    # Define path to data\n",
    "    # Note: Path relative to KHIEM's files, make changes to relative path if necessary\n",
    "    dataPath = 'BeerAdvocatePA4/' + fname\n",
    "    \n",
    "    # Read csv into pandas frame\n",
    "    df = pd.read_csv(dataPath)\n",
    "    \n",
    "    # Return frame\n",
    "    return df\n",
    "\n",
    "def char2oh(padded,translate, beers):\n",
    "    # Each row has form: beer style index||overall||character indices\n",
    "    # TODO: Onehot the beer style and concatenate to overall\n",
    "    # Onehot the character indices and concatenate the beerstyle||overall to each onehotted char\n",
    "    \n",
    "    # Each row's beer has form 1x#ofpossiblebeers\n",
    "    beerstyles = np.zeros((padded.shape[0],len(beers)))\n",
    "    # Each row's review has form max(lenofsequence) x #ofpossiblecharacters\n",
    "    # Since we padded each review, they all have the same number of characters\n",
    "    # Subtract two since we know the first two values aren't characters\n",
    "    chars = np.zeros((padded.shape[0], (padded.shape[1] - 2), len(translate)))\n",
    "    \n",
    "    # First two columns are beerstyle indices and overalls\n",
    "    bsidx = padded[:,0]\n",
    "    ovrl = padded[:,1]\n",
    "    \n",
    "    # The rest are characters\n",
    "    ch = padded[:,2:]\n",
    "    \n",
    "    # Index with bsidx\n",
    "    beerstyles[np.arange(padded.shape[0]), bsidx.astype(int)] = 1\n",
    "    \n",
    "    igrid = np.mgrid[0:padded.shape[0], 0:(padded.shape[1]-2)]\n",
    "    # Index with ch, we use meshgrid since this is a 3d array\n",
    "    chars[igrid[0], igrid[1], ch.astype(int)] = 1\n",
    "    \n",
    "    # Concatenate overall and beer style\n",
    "    meta_data = np.c_[ovrl, beerstyles]\n",
    "    \n",
    "    # Tile and reshape meta_data so we have a copy for each one of the characters\n",
    "    tiled_meta = np.tile(meta_data, padded.shape[1] - 2).reshape(padded.shape[0], (padded.shape[1] - 2), -1)\n",
    "    \n",
    "    # Concatenate the items \n",
    "    \n",
    "    # Return both the concatenated and just the one hot\n",
    "    return np.c_[tiled_meta, chars],ch\n",
    "    \n",
    "\n",
    "def process_train_data(data):\n",
    "    # TODO: Input is a pandas DataFrame and return a numpy array (or a torch Tensor/ Variable)\n",
    "    # that has all features (including characters in one hot encoded form).\n",
    "\n",
    "    # Get the dictionary to translate between ASCII and onehot index\n",
    "    with open(\"ASCII2oneHot.pkl\", \"rb\") as f:\n",
    "        translate = pickle.load(f)\n",
    "    \n",
    "    # Get the dictionary to translate between beer style and index\n",
    "    with open(\"BeerDict.pkl\", \"rb\") as f:\n",
    "        beers = pickle.load(f)\n",
    "        \n",
    "    # List of reviews to onehot after translation\n",
    "    toOnehot = []\n",
    "    \n",
    "    # For each review, convert to list of its translated characters\n",
    "    # Translated means ord(c) -> onehot index\n",
    "    # Also translate the beer style to its index value\n",
    "    # Concatenate all the data and convert to tensor\n",
    "    for idx,rev in data.iterrows():\n",
    "        if isinstance(rev['review/text'], str):\n",
    "            toOnehot.append(torch.Tensor([beers[rev['beer/style']]] + [rev['review/overall']] + \n",
    "                                         [translate[ord(x)] for x in list(chr(0)+rev['review/text']+chr(1))]))\n",
    "    \n",
    "    # Pad all smaller sentences with 1s to signify <EOS>\n",
    "    padded = pad_data(toOnehot, translate[1])\n",
    "    del toOnehot\n",
    "\n",
    "    # Take the array padded sentences and one-hot the characters.\n",
    "    # Beer style also gets one-hot\n",
    "    # Overall does not\n",
    "    reviews,labels = char2oh(np.array(padded), translate, beers)\n",
    "    del padded\n",
    "    \n",
    "    # Since the labels are simply the next characters, we take all characters except the last one\n",
    "    # for the review, and everything but the first one for the labels\n",
    "    return torch.Tensor(reviews[:,0:-1,:]), torch.Tensor(labels[:,1:]).type(torch.LongTensor)\n",
    "    \n",
    "    \n",
    "def train_valid_split(data):\n",
    "    # TODO: Takes in train data as dataframe and\n",
    "    # splits it into training and validation data.\n",
    "    \n",
    "    # List of indices of the data\n",
    "    ind = np.arange(len(data))\n",
    "    \n",
    "    # Randomize the split\n",
    "    np.random.shuffle(ind)\n",
    "    \n",
    "    # Where to split the indices\n",
    "    # We'll take first 20% for validation, the rest for training\n",
    "    split = int(0.2*len(data))\n",
    "    \n",
    "    # Split the indices\n",
    "    vIndices = ind[0:split]\n",
    "    tIndices = ind[split:]\n",
    "    \n",
    "    # Group the indices into their frames then return those\n",
    "    validation_data = data.iloc[vIndices]\n",
    "    train_data = data.iloc[tIndices]\n",
    "    \n",
    "    return train_data,validation_data\n",
    "    \n",
    "def process_test_data(data):\n",
    "    # TODO: Takes in pandas DataFrame and returns a numpy array (or a torch Tensor/ Variable)\n",
    "    # that has all input features. Note that test data does not contain any review so you don't\n",
    "    # have to worry about one hot encoding the data.\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "def pad_data(orig_data, pad):\n",
    "    # TODO: Since you will be training in batches and training sample of each batch may have reviews\n",
    "    # of varying lengths, you will need to pad your data so that all samples have reviews of length\n",
    "    # equal to the longest review in a batch. You will pad all the sequences with <EOS> character \n",
    "    # representation in one hot encoding.\n",
    "    # Data comes in as translated ASCII representation, simply sort and call torch pad\n",
    "    return torch.nn.utils.rnn.pad_sequence(sorted(orig_data, key = lambda x: len(x), reverse=True), \n",
    "                                           batch_first=True, padding_value=pad)\n",
    "    \n",
    "\n",
    "def getBatchIter(data, batchSize):\n",
    "    # TODO: Returns a list of batches of indices\n",
    "    # The list of batch indices will be used to index into the\n",
    "    # corresponding data frame to extract the data\n",
    "    \n",
    "    # List of all possible indices\n",
    "    ind = np.arange(len(data))\n",
    "    \n",
    "    # Calculate how many batches of batchSize would fit into\n",
    "    # into the length of the data\n",
    "    numBatches = int(len(data)/batchSize)\n",
    "    \n",
    "    # Split the array of indices into roughly equivalent batch sized batches\n",
    "    batchedInd = np.array_split(ind, numBatches)\n",
    "    \n",
    "    return batchedInd\n",
    "    \n",
    "def validate(model, validIter, X_valid):\n",
    "    # TODO: Run the model on the entire validation set for loss\n",
    "    # Loss\n",
    "    Criterion = torch.nn.CrossEntropyLoss()\n",
    "    # No need for gradient\n",
    "    with torch.no_grad():\n",
    "        totalLoss = 0\n",
    "        # Validation loop\n",
    "        for batch_count, batchInd in enumerate(validIter, 0):\n",
    "             # Get the dataframe for the batch\n",
    "            batchFrame = X_valid.iloc[batchInd]\n",
    "\n",
    "            # Process the batch for data and labels\n",
    "            batch, labels = process_train_data(batchFrame)\n",
    "            batch, labels = batch.to(computing_device), labels.to(computing_device)\n",
    "            \n",
    "            # Run our batch through the model\n",
    "            # batch has shape Batchsize x Seqlen x Input Dim\n",
    "            output, (h,c) = model(batch)\n",
    "            \n",
    "            # Save space\n",
    "            del h\n",
    "            del c\n",
    "            \n",
    "            # Reshape the output and labels to so that the loss function\n",
    "            # can simply interpret time as another batch\n",
    "            # This will be fine since sum of sum can be thought of as just\n",
    "            # one sum\n",
    "            output = output.contiguous().view(-1, output.shape[2])\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Get loss and compute gradients\n",
    "            loss = Criterion(output,labels)\n",
    "            totalLoss += float(loss)\n",
    "            \n",
    "            # Progress bar\n",
    "            if batch_count % 50 == 0:\n",
    "                print(\"Validation: On batch %d\" % (batch_count))\n",
    "                \n",
    "        # Return the average loss over the batch count\n",
    "        return totalLoss/(batch_count+1)\n",
    "            \n",
    "\n",
    "def train(model, X_train, X_valid, cfg):\n",
    "    # TODO: Train the model!\n",
    "    # Datas are given as pandas data frame. Call process on-line as we train to\n",
    "    # get the data and label\n",
    "    \n",
    "    epochs = np.arange(cfg['epochs'])\n",
    "    l_rate = cfg['learning_rate']\n",
    "    penalty = cfg['L2_penalty']\n",
    "    \n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    Criterion = torch.nn.CrossEntropyLoss() # We'll use NLL\n",
    "    Optimizer = optim.Adam(model.parameters(), lr=l_rate, weight_decay=penalty) # Let's use ADAM\n",
    "    \n",
    "    # Size of each batch\n",
    "    batchSize = 150\n",
    "    \n",
    "    # Create the batch iterator for the data\n",
    "    trainIter = getBatchIter(X_train, batchSize)\n",
    "    validIter = getBatchIter(X_valid, batchSize)\n",
    "    \n",
    "    all_loss = []\n",
    "    v_loss = []\n",
    "    # Training loop\n",
    "    for e in epochs:\n",
    "        batch_loss = 0\n",
    "        for batch_count, batchInd in enumerate(trainIter,0):\n",
    "            # Get the dataframe for the batch\n",
    "            batchFrame = X_train.iloc[batchInd]\n",
    "\n",
    "            # Process the batch for data and labels\n",
    "            batch, labels = process_train_data(batchFrame)\n",
    "            batch, labels = batch.to(computing_device), labels.to(computing_device)\n",
    "\n",
    "            # Run our batch through the model\n",
    "            # batch has shape Batchsize x Seqlen x Input Dim\n",
    "            output, (h,c) = model(batch)\n",
    "            \n",
    "            # Save space\n",
    "            del h\n",
    "            del c\n",
    "            \n",
    "            # Reshape the output and labels to so that the loss function\n",
    "            # can simply interpret time as another batch\n",
    "            # This will be fine since sum of sum can be thought of as just\n",
    "            # one sum\n",
    "\n",
    "            output = output.contiguous().view(-1, output.shape[2])\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Get loss and compute gradients\n",
    "            loss = Criterion(output,labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimize step\n",
    "            Optimizer.step()\n",
    "            batch_loss += float(loss)\n",
    "\n",
    "            # Progress bar\n",
    "            if batch_count % 50 == 0:\n",
    "                batch_loss /= 50\n",
    "                print(\"On batch %d with loss %f\" % (batch_count, batch_loss))\n",
    "                all_loss.append(batch_loss)\n",
    "                batch_loss = 0\n",
    "                \n",
    "            # TODO: Implement validation\n",
    "            if batch_count % 3000 == 0:\n",
    "                # Validate and save\n",
    "                vloss = validate(model, validIter, X_valid)\n",
    "                print(\"Validation on epoch %d on batch % has loss %f\" % (e,batch_count,vloss))\n",
    "                v_loss.append(vloss)\n",
    "                \n",
    "                # Model checkpoint\n",
    "                torch.save(model.state_dict(), 'ModelCheckpoints/BaseLSTM'+str(batch_count+1)+'.mdl')           \n",
    "            \n",
    "        print(\"Completed epoch %d\" % e)\n",
    "    \n",
    "    print(\"Completed Training\")\n",
    "        \n",
    "    \n",
    "    \n",
    "def generate(model, X_test, cfg):\n",
    "    # TODO: Given n rows in test data, generate a list of n strings, where each string is the review\n",
    "    # corresponding to each input row in test data.\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    \n",
    "def save_to_file(outputs, fname):\n",
    "    # TODO: Given the list of generated review outputs and output file name, save all these reviews to\n",
    "    # the file in .txt format.\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch 0 with loss 0.093163\n",
      "Validation: On batch 0\n",
      "Validation: On batch 50\n",
      "Validation: On batch 100\n",
      "Validation: On batch 150\n",
      "Validation: On batch 200\n",
      "Validation: On batch 250\n",
      "Validation: On batch 300\n",
      "Validation: On batch 350\n",
      "Validation: On batch 400\n",
      "Validation: On batch 450\n",
      "Validation: On batch 500\n",
      "Validation: On batch 550\n",
      "Validation: On batch 600\n",
      "Validation: On batch 650\n",
      "Validation: On batch 700\n",
      "Validation: On batch 750\n",
      "Validation: On batch 800\n",
      "Validation: On batch 850\n",
      "Validation: On batch 900\n",
      "Validation: On batch 950\n",
      "Validation: On batch 1000\n",
      "Validation: On batch 1050\n",
      "Validation: On batch 1100\n",
      "Validation: On batch 1150\n",
      "Validation: On batch 1200\n",
      "Validation: On batch 1250\n",
      "Validation: On batch 1300\n",
      "Validation: On batch 1350\n",
      "Validation: On batch 1400\n",
      "Validation: On batch 1450\n",
      "Validation: On batch 1500\n",
      "Validation: On batch 1550\n",
      "Validation: On batch 1600\n",
      "Validation: On batch 1650\n",
      "Validation on epoch 0 on batch 0s loss 4.637653\n",
      "On batch 50 with loss 4.071425\n",
      "On batch 100 with loss 3.602895\n",
      "On batch 150 with loss 3.548877\n",
      "On batch 200 with loss 3.554931\n",
      "On batch 250 with loss 3.549045\n",
      "On batch 300 with loss 3.547931\n",
      "On batch 350 with loss 3.542157\n",
      "On batch 400 with loss 3.550015\n",
      "On batch 450 with loss 3.540105\n",
      "On batch 500 with loss 3.549413\n",
      "On batch 550 with loss 3.534483\n",
      "On batch 600 with loss 3.546000\n",
      "On batch 650 with loss 3.534251\n",
      "On batch 700 with loss 3.544599\n",
      "On batch 750 with loss 3.537289\n",
      "On batch 800 with loss 3.549540\n",
      "On batch 850 with loss 3.549286\n",
      "On batch 900 with loss 3.543929\n",
      "On batch 950 with loss 3.542756\n",
      "On batch 1000 with loss 3.541329\n",
      "On batch 1050 with loss 3.547357\n",
      "On batch 1100 with loss 3.540002\n",
      "On batch 1150 with loss 3.557677\n",
      "On batch 1200 with loss 3.543006\n",
      "On batch 1250 with loss 3.548567\n",
      "On batch 1300 with loss 3.540309\n",
      "On batch 1350 with loss 3.550906\n",
      "On batch 1400 with loss 3.539006\n",
      "On batch 1450 with loss 3.537222\n",
      "On batch 1500 with loss 3.552139\n",
      "On batch 1550 with loss 3.549318\n",
      "On batch 1600 with loss 3.544312\n",
      "On batch 1650 with loss 3.545917\n",
      "On batch 1700 with loss 3.561123\n",
      "On batch 1750 with loss 3.550988\n",
      "On batch 1800 with loss 3.554755\n",
      "On batch 1850 with loss 3.549446\n",
      "On batch 1900 with loss 3.540719\n",
      "On batch 1950 with loss 3.552424\n",
      "On batch 2000 with loss 3.551494\n",
      "On batch 2050 with loss 3.540205\n",
      "On batch 2100 with loss 3.543198\n",
      "On batch 2150 with loss 3.554493\n",
      "On batch 2200 with loss 3.535918\n",
      "On batch 2250 with loss 3.540578\n",
      "On batch 2300 with loss 3.548282\n",
      "On batch 2350 with loss 3.550464\n",
      "On batch 2400 with loss 3.543712\n",
      "On batch 2450 with loss 3.548282\n",
      "On batch 2500 with loss 3.553196\n",
      "On batch 2550 with loss 3.546672\n",
      "On batch 2600 with loss 3.543497\n",
      "On batch 2650 with loss 3.543814\n",
      "On batch 2700 with loss 3.538464\n",
      "On batch 2750 with loss 3.549917\n",
      "On batch 2800 with loss 3.545392\n",
      "On batch 2850 with loss 3.540974\n",
      "On batch 2900 with loss 3.560624\n",
      "On batch 2950 with loss 3.545275\n",
      "On batch 3000 with loss 3.551172\n",
      "Validation: On batch 0\n",
      "Validation: On batch 50\n",
      "Validation: On batch 100\n",
      "Validation: On batch 150\n",
      "Validation: On batch 200\n",
      "Validation: On batch 250\n",
      "Validation: On batch 300\n",
      "Validation: On batch 350\n",
      "Validation: On batch 400\n",
      "Validation: On batch 450\n",
      "Validation: On batch 500\n",
      "Validation: On batch 550\n",
      "Validation: On batch 600\n",
      "Validation: On batch 650\n",
      "Validation: On batch 700\n",
      "Validation: On batch 750\n",
      "Validation: On batch 800\n",
      "Validation: On batch 850\n",
      "Validation: On batch 900\n",
      "Validation: On batch 950\n",
      "Validation: On batch 1000\n",
      "Validation: On batch 1050\n",
      "Validation: On batch 1100\n",
      "Validation: On batch 1150\n",
      "Validation: On batch 1200\n",
      "Validation: On batch 1250\n",
      "Validation: On batch 1300\n",
      "Validation: On batch 1350\n",
      "Validation: On batch 1400\n",
      "Validation: On batch 1450\n",
      "Validation: On batch 1500\n",
      "Validation: On batch 1550\n",
      "Validation: On batch 1600\n",
      "Validation: On batch 1650\n",
      "Validation on epoch 0 on batch 3000s loss 3.546971\n",
      "On batch 3050 with loss 3.548052\n",
      "On batch 3100 with loss 3.546053\n",
      "On batch 3150 with loss 3.543225\n",
      "On batch 3200 with loss 3.554839\n",
      "On batch 3250 with loss 3.544514\n",
      "On batch 3300 with loss 3.548093\n",
      "On batch 3350 with loss 3.553029\n",
      "On batch 3400 with loss 3.548697\n",
      "On batch 3450 with loss 3.550345\n",
      "On batch 3500 with loss 3.551063\n",
      "On batch 3550 with loss 3.555881\n",
      "On batch 3600 with loss 3.546943\n",
      "On batch 3650 with loss 3.538648\n",
      "On batch 3700 with loss 3.548283\n",
      "On batch 3750 with loss 3.547414\n",
      "On batch 3800 with loss 3.555766\n",
      "On batch 3850 with loss 3.544544\n",
      "On batch 3900 with loss 3.544633\n",
      "On batch 3950 with loss 3.548347\n",
      "On batch 4000 with loss 3.548862\n",
      "On batch 4050 with loss 3.542680\n",
      "On batch 4100 with loss 3.556218\n",
      "On batch 4150 with loss 3.549592\n",
      "On batch 4200 with loss 3.542912\n",
      "On batch 4250 with loss 3.546890\n",
      "On batch 4300 with loss 3.542513\n",
      "On batch 4350 with loss 3.547821\n",
      "On batch 4400 with loss 3.554577\n",
      "On batch 4450 with loss 3.547659\n",
      "On batch 4500 with loss 3.543036\n",
      "On batch 4550 with loss 3.545681\n",
      "On batch 4600 with loss 3.552745\n",
      "On batch 4650 with loss 3.536552\n",
      "On batch 4700 with loss 3.547716\n",
      "On batch 4750 with loss 3.542939\n",
      "On batch 4800 with loss 3.552753\n",
      "On batch 4850 with loss 3.548220\n",
      "On batch 4900 with loss 3.543971\n",
      "On batch 4950 with loss 3.546270\n",
      "On batch 5000 with loss 3.551199\n",
      "On batch 5050 with loss 3.546560\n",
      "On batch 5100 with loss 3.553074\n",
      "On batch 5150 with loss 3.556685\n",
      "On batch 5200 with loss 3.538064\n",
      "On batch 5250 with loss 3.551896\n",
      "On batch 5300 with loss 3.543441\n",
      "On batch 5350 with loss 3.549421\n",
      "On batch 5400 with loss 3.547514\n",
      "On batch 5450 with loss 3.539649\n",
      "On batch 5500 with loss 3.536492\n",
      "On batch 5550 with loss 3.540932\n",
      "On batch 5600 with loss 3.541992\n",
      "On batch 5650 with loss 3.549602\n",
      "On batch 5700 with loss 3.550722\n",
      "On batch 5750 with loss 3.542412\n",
      "On batch 5800 with loss 3.544403\n",
      "On batch 5850 with loss 3.540714\n",
      "On batch 5900 with loss 3.539958\n",
      "On batch 5950 with loss 3.544059\n",
      "On batch 6000 with loss 3.554773\n",
      "Validation: On batch 0\n",
      "Validation: On batch 50\n",
      "Validation: On batch 100\n",
      "Validation: On batch 150\n",
      "Validation: On batch 200\n",
      "Validation: On batch 250\n",
      "Validation: On batch 300\n",
      "Validation: On batch 350\n",
      "Validation: On batch 400\n",
      "Validation: On batch 450\n",
      "Validation: On batch 500\n",
      "Validation: On batch 550\n",
      "Validation: On batch 600\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_data_fname = \"Beeradvocate_Train.csv\"\n",
    "    test_data_fname = \"Beeradvocate_Test.csv\"\n",
    "    out_fname = \"model_outputs.out\"\n",
    "    \n",
    "    train_data = load_data(train_data_fname) # Generating the pandas DataFrame\n",
    "    test_data = load_data(test_data_fname) # Generating the pandas DataFrame\n",
    "    X_train, X_valid = train_valid_split(train_data) # Splitting the train data into train-valid data\n",
    "    \n",
    "    model = baselineLSTM(cfg) # Replace this with model = <your model name>(cfg)\n",
    "    if cfg['cuda']:\n",
    "        computing_device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        computing_device = torch.device(\"cpu\")\n",
    "    model.to(computing_device)\n",
    "    \n",
    "    train(model, X_train, X_valid, cfg) # Train the model\n",
    "    outputs = generate(model, X_test, cfg) # Generate the outputs for test data\n",
    "    save_to_file(outputs, out_fname) # Save the generated outputs to a file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
