{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    # From the csv file given by filename and return a pandas DataFrame of the read csv.\n",
    "    \n",
    "    # Define path to data\n",
    "    # Note: Path relative to KHIEM's files, make changes to relative path if necessary\n",
    "    dataPath = 'BeerAdvocatePA4/' + fname\n",
    "    \n",
    "    # Read csv into pandas frame\n",
    "    df = pd.read_csv(dataPath)\n",
    "    \n",
    "    # Return frame\n",
    "    return df\n",
    "\n",
    "def char2oh(padded,translate, beers):\n",
    "    # Each row has form: beer style index||overall||character indices\n",
    "    # TODO: Onehot the beer style and concatenate to overall\n",
    "    # Onehot the character indices and concatenate the beerstyle||overall to each onehotted char\n",
    "    \n",
    "    # Each row's beer has form 1x#ofpossiblebeers\n",
    "    beerstyles = np.zeros((padded.shape[0],len(beers)))\n",
    "    # Each row's review has form max(lenofsequence) x #ofpossiblecharacters\n",
    "    # Since we padded each review, they all have the same number of characters\n",
    "    # Subtract two since we know the first two values aren't characters\n",
    "    chars = np.zeros((padded.shape[0], (padded.shape[1] - 2), len(translate)))\n",
    "    \n",
    "    # First two columns are beerstyle indices and overalls\n",
    "    bsidx = padded[:,0]\n",
    "    ovrl = padded[:,1]\n",
    "    \n",
    "    # The rest are characters\n",
    "    ch = padded[:,2:]\n",
    "    \n",
    "    # Index with bsidx\n",
    "    beerstyles[np.arange(padded.shape[0]), bsidx.astype(int)] = 1\n",
    "    \n",
    "    igrid = np.mgrid[0:padded.shape[0], 0:(padded.shape[1]-2)]\n",
    "    # Index with ch, we use meshgrid since this is a 3d array\n",
    "    chars[igrid[0], igrid[1], ch.astype(int)] = 1\n",
    "    \n",
    "    # Concatenate overall and beer style\n",
    "    meta_data = np.c_[ovrl, beerstyles]\n",
    "    \n",
    "    # Tile and reshape meta_data so we have a copy for each one of the characters\n",
    "    tiled_meta = np.tile(meta_data, padded.shape[1] - 2).reshape(padded.shape[0], (padded.shape[1] - 2), -1)\n",
    "    \n",
    "    # Concatenate the items \n",
    "    \n",
    "    # Return both the concatenated and just the one hot\n",
    "    return np.c_[tiled_meta, chars],ch\n",
    "    \n",
    "\n",
    "def process_train_data(data):\n",
    "    # TODO: Input is a pandas DataFrame and return a numpy array (or a torch Tensor/ Variable)\n",
    "    # that has all features (including characters in one hot encoded form).\n",
    "\n",
    "    # Get the dictionary to translate between ASCII and onehot index\n",
    "    with open(\"ASCII2oneHot.pkl\", \"rb\") as f:\n",
    "        translate = pickle.load(f)\n",
    "    \n",
    "    # Get the dictionary to translate between beer style and index\n",
    "    with open(\"BeerDict.pkl\", \"rb\") as f:\n",
    "        beers = pickle.load(f)\n",
    "        \n",
    "    # List of reviews to onehot after translation\n",
    "    toOnehot = []\n",
    "    \n",
    "    # For each review, convert to list of its translated characters\n",
    "    # Translated means ord(c) -> onehot index\n",
    "    # Also translate the beer style to its index value\n",
    "    # Concatenate all the data and convert to tensor\n",
    "    for idx,rev in data.iterrows():\n",
    "        if isinstance(rev['review/text'], str):\n",
    "            toOnehot.append(torch.Tensor([beers[rev['beer/style']]] + [rev['review/overall']] + \n",
    "                                         [translate[ord(x)] for x in list(chr(0)+rev['review/text']+chr(1))]))\n",
    "    \n",
    "    # No need to pad, we can just stack \n",
    "    padded = pad_data(toOnehot, translate[1])\n",
    "    del toOnehot\n",
    "\n",
    "    # Take the array padded sentences and one-hot the characters.\n",
    "    # Beer style also gets one-hot\n",
    "    # Overall does not\n",
    "    reviews,labels = char2oh(np.array(padded), translate, beers)\n",
    "    del padded\n",
    "    \n",
    "    # Since the labels are simply the next characters, we take all characters except the last one\n",
    "    # for the review, and everything but the first one for the labels\n",
    "    return torch.Tensor(reviews[:,0:-1,:]), torch.Tensor(labels[:,1:]).type(torch.LongTensor)\n",
    "    \n",
    "    \n",
    "def train_valid_split(data):\n",
    "    # TODO: Takes in train data as dataframe and\n",
    "    # splits it into training and validation data.\n",
    "    \n",
    "    # List of indices of the data\n",
    "    ind = np.arange(len(data))\n",
    "    \n",
    "    # Randomize the split\n",
    "    np.random.shuffle(ind)\n",
    "    \n",
    "    # Where to split the indices\n",
    "    # We'll take first 20% for validation, the rest for training\n",
    "    split = int(0.2*len(data))\n",
    "    \n",
    "    # Split the indices\n",
    "    vIndices = ind[0:split]\n",
    "    tIndices = ind[split:]\n",
    "    \n",
    "    # Group the indices into their frames then return those\n",
    "    validation_data = data.iloc[vIndices]\n",
    "    train_data = data.iloc[tIndices]\n",
    "    \n",
    "    return train_data,validation_data\n",
    "    \n",
    "def process_test_data(data):\n",
    "    # TODO: Takes in pandas DataFrame and returns a numpy array (or a torch Tensor/ Variable)\n",
    "    # that has all input features. Note that test data does not contain any review so you don't\n",
    "    # have to worry about one hot encoding the data.\n",
    "    \n",
    "    # Get the dictionary to translate between ASCII and onehot index\n",
    "    with open(\"ASCII2oneHot.pkl\", \"rb\") as f:\n",
    "        translate = pickle.load(f)\n",
    "    \n",
    "    # Get the dictionary to translate between beer style and index\n",
    "    with open(\"BeerDict.pkl\", \"rb\") as f:\n",
    "        beers = pickle.load(f)\n",
    "        \n",
    "    tostk = []\n",
    "    # Take each row and establish the metadata||<SOS> \n",
    "    for idx,rev in data.iterrows():\n",
    "        tostk.append(torch.Tensor([beers[rev['beer/style']]] + [rev['review/overall']] + \n",
    "                                     [translate[0]]))\n",
    "                        \n",
    "    # Stack the tensors\n",
    "    stked = torch.stack(tostk)\n",
    "    del tostk\n",
    "    \n",
    "    # Pass back the meta data to concatenate in each time step\n",
    "    orig = stked\n",
    "    stked, start = char2oh(np.array(stked), translate, beers)\n",
    "    \n",
    "    return torch.Tensor(stked), orig\n",
    "        \n",
    "\n",
    "    \n",
    "def pad_data(orig_data, pad):\n",
    "    # TODO: Since you will be training in batches and training sample of each batch may have reviews\n",
    "    # of varying lengths, you will need to pad your data so that all samples have reviews of length\n",
    "    # equal to the longest review in a batch. You will pad all the sequences with <EOS> character \n",
    "    # representation in one hot encoding.\n",
    "    # Data comes in as translated ASCII representation, simply sort and call torch pad\n",
    "    return torch.nn.utils.rnn.pad_sequence(sorted(orig_data, key = lambda x: len(x), reverse=True), \n",
    "                                           batch_first=True, padding_value=pad)\n",
    "    \n",
    "\n",
    "def getBatchIter(data, batchSize):\n",
    "    # TODO: Returns a list of batches of indices\n",
    "    # The list of batch indices will be used to index into the\n",
    "    # corresponding data frame to extract the data\n",
    "    \n",
    "    # List of all possible indices\n",
    "    ind = np.arange(len(data))\n",
    "    \n",
    "    # Calculate how many batches of batchSize would fit into\n",
    "    # into the length of the data\n",
    "    numBatches = int(len(data)/batchSize)\n",
    "    \n",
    "    # Split the array of indices into roughly equivalent batch sized batches\n",
    "    batchedInd = np.array_split(ind, numBatches)\n",
    "    \n",
    "    return batchedInd\n",
    "    \n",
    "def validate(model, validIter, X_valid):\n",
    "    # TODO: Run the model on the entire validation set for loss\n",
    "    # Loss\n",
    "    Criterion = torch.nn.CrossEntropyLoss()\n",
    "    # No need for gradient\n",
    "    with torch.no_grad():\n",
    "        totalLoss = 0\n",
    "        # Validation loop\n",
    "        for batch_count, batchInd in enumerate(validIter, 0):\n",
    "             # Get the dataframe for the batch\n",
    "            batchFrame = X_valid.iloc[batchInd]\n",
    "\n",
    "            # Process the batch for data and labels\n",
    "            batch, labels = process_train_data(batchFrame)\n",
    "            batch, labels = batch.to(computing_device), labels.to(computing_device)\n",
    "            \n",
    "            # Run our batch through the model\n",
    "            # batch has shape Batchsize x Seqlen x Input Dim\n",
    "            output, (h,c) = model(batch)\n",
    "            \n",
    "            # Save space\n",
    "            del h\n",
    "            del c\n",
    "            \n",
    "            # Reshape the output and labels to so that the loss function\n",
    "            # can simply interpret time as another batch\n",
    "            # This will be fine since sum of sum can be thought of as just\n",
    "            # one sum\n",
    "            output = output.contiguous().view(-1, output.shape[2])\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Get loss and compute gradients\n",
    "            loss = Criterion(output,labels)\n",
    "            totalLoss += float(loss)\n",
    "            \n",
    "            # Progress bar\n",
    "            if batch_count % 50 == 0:\n",
    "                print(\"Validation: On batch %d\" % (batch_count))\n",
    "                \n",
    "        # Return the average loss over the batch count\n",
    "        return totalLoss/(batch_count+1)\n",
    "            \n",
    "\n",
    "def train(model, X_train, X_valid, cfg):\n",
    "    # TODO: Train the model!\n",
    "    # Datas are given as pandas data frame. Call process on-line as we train to\n",
    "    # get the data and label\n",
    "    \n",
    "    epochs = np.arange(cfg['epochs'])\n",
    "    l_rate = cfg['learning_rate']\n",
    "    penalty = cfg['L2_penalty']\n",
    "    \n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    Criterion = torch.nn.CrossEntropyLoss() # We'll use cross entropy\n",
    "    Optimizer = optim.Adam(model.parameters(), lr=l_rate, weight_decay=penalty) # Let's use ADAM\n",
    "    \n",
    "    # Size of each batch\n",
    "    batchSize = 150\n",
    "    \n",
    "    # Create the batch iterator for the data\n",
    "    trainIter = getBatchIter(X_train, batchSize)\n",
    "    validIter = getBatchIter(X_valid, batchSize)\n",
    "    \n",
    "    all_loss = []\n",
    "    v_loss = []\n",
    "    # Training loop\n",
    "    for e in epochs:\n",
    "        batch_loss = 0\n",
    "        for batch_count, batchInd in enumerate(trainIter,0):\n",
    "            # Get the dataframe for the batch\n",
    "            batchFrame = X_train.iloc[batchInd]\n",
    "\n",
    "            # Process the batch for data and labels\n",
    "            batch, labels = process_train_data(batchFrame)\n",
    "            batch, labels = batch.to(computing_device), labels.to(computing_device)\n",
    "\n",
    "            # Run our batch through the model\n",
    "            # batch has shape Batchsize x Seqlen x Input Dim\n",
    "            output, (h,c) = model(batch)\n",
    "            \n",
    "            # Save space\n",
    "            del h\n",
    "            del c\n",
    "            \n",
    "            # Reshape the output and labels to so that the loss function\n",
    "            # can simply interpret time as another batch\n",
    "            # This will be fine since sum of sum can be thought of as just\n",
    "            # one sum\n",
    "\n",
    "            output = output.contiguous().view(-1, output.shape[2])\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            Optimizer.zero_grad()\n",
    "            # Get loss and compute gradients\n",
    "            loss = Criterion(output,labels)\n",
    "            loss.backward()\n",
    "            \n",
    "          \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            \n",
    "            # Optimize step\n",
    "            Optimizer.step()\n",
    "            batch_loss += float(loss)\n",
    "\n",
    "            # Progress bar\n",
    "            if batch_count % 50 == 0:\n",
    "                batch_loss /= 50\n",
    "                print(\"On batch %d with loss %f\" % (batch_count, batch_loss))\n",
    "                all_loss.append(batch_loss)\n",
    "                batch_loss = 0\n",
    "                \n",
    "            # TODO: Implement validation\n",
    "            if batch_count % 3500 == 0:\n",
    "                # Validate and save\n",
    "                #vloss = validate(model, validIter, X_valid)\n",
    "                #print(\"Validation on epoch %d on batch % has loss %f\" % (e,batch_count,vloss))\n",
    "                #v_loss.append(vloss)\n",
    "                \n",
    "                # Model checkpoint\n",
    "                torch.save(model.state_dict(), 'ModelCheckpoints/BaseLSTM'+str(batch_count+1)+'.mdl')           \n",
    "            \n",
    "        print(\"Completed epoch %d\" % e)\n",
    "    \n",
    "    print(\"Completed Training\")\n",
    "        \n",
    "    \n",
    "    \n",
    "def generate(model, X_test, cfg):\n",
    "    # TODO: Given n rows in test data, generate a list of n strings, where each string is the review\n",
    "    # corresponding to each input row in test data.\n",
    "    \n",
    "    # Get the dictionary to translate between ASCII and onehot index\n",
    "    with open(\"ASCII2oneHot.pkl\", \"rb\") as f:\n",
    "        translate = pickle.load(f)\n",
    "    \n",
    "    # Get the dictionary to translate between beer style and index\n",
    "    with open(\"BeerDict.pkl\", \"rb\") as f:\n",
    "        beers = pickle.load(f)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get iterator\n",
    "        testIter = getBatchIter(X_test, batchSize = 150)\n",
    "    \n",
    "        for batch_count, batchInd in enumerate(testIter, 0):\n",
    "             # Get the dataframe for the batch\n",
    "            batchFrame = X_test.iloc[batchInd]\n",
    "\n",
    "            # Process the batch for test data\n",
    "            # Orig is the array of meta data and <SOS> character\n",
    "            batch, orig = process_test_data(batchFrame)\n",
    "            batch = batch.to(computing_device)\n",
    "            \n",
    "            # Starting characters and meta datas for the next time step\n",
    "            start = orig[-1]\n",
    "            metas = orig[0:2]\n",
    "            \n",
    "            # The generated strings so far\n",
    "            generated = start\n",
    "            \n",
    "            # While not all batches have at least one EOS we continue generating.\n",
    "            while(not(generated == 110).any(axis=1).all()):\n",
    "                # Batch is in the shape of batchSize x 1 x input dim\n",
    "                # Feed the batch to get the outputs\n",
    "                probs = model(batch)\n",
    "\n",
    "\n",
    "                # Softmax over the input dim to get the probabilities\n",
    "                # Divide by temperature to implement the temperature softmax\n",
    "                probs = torch.nn.functional.softmax(probs/cfg['gen_temp'], dim = 2)\n",
    "\n",
    "                # Sample from our batchsize of probabilities to get batchsize of next output\n",
    "                sampled = np.array(torch.distributions.Categorical(probs).sample())\n",
    "                \n",
    "                # Concatenate the sampled to our generated\n",
    "                generated = np.c_[generated,sampled]\n",
    "                \n",
    "                # Make our next input\n",
    "                batch = char2oh(np.c_[metas, sampled], translate, beers)\n",
    "            \n",
    "        \n",
    "        return generated\n",
    "       \n",
    "        \n",
    "    \n",
    "    \n",
    "def save_to_file(outputs, fname):\n",
    "    # TODO: Given the list of generated review outputs and output file name, save all these reviews to\n",
    "    # the file in .txt format.\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.nn.LSTM(5,11, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((3,4,5))\n",
    "h = torch.zeros((1,3,11))\n",
    "c = torch.zeros((1,3,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([44, 5])"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0239,  0.0242, -0.0081,  0.0406,  0.0170,  0.0417,  0.0389,\n",
       "            0.0124, -0.0506, -0.0275, -0.0616],\n",
       "          [-0.0438,  0.0301, -0.0079,  0.0586,  0.0195,  0.0659,  0.0610,\n",
       "            0.0259, -0.0742, -0.0319, -0.0928],\n",
       "          [-0.0575,  0.0293, -0.0051,  0.0652,  0.0189,  0.0803,  0.0740,\n",
       "            0.0359, -0.0861, -0.0300, -0.1098],\n",
       "          [-0.0663,  0.0268, -0.0017,  0.0668,  0.0183,  0.0890,  0.0817,\n",
       "            0.0424, -0.0923, -0.0268, -0.1195]],\n",
       " \n",
       "         [[-0.0239,  0.0242, -0.0081,  0.0406,  0.0170,  0.0417,  0.0389,\n",
       "            0.0124, -0.0506, -0.0275, -0.0616],\n",
       "          [-0.0438,  0.0301, -0.0079,  0.0586,  0.0195,  0.0659,  0.0610,\n",
       "            0.0259, -0.0742, -0.0319, -0.0928],\n",
       "          [-0.0575,  0.0293, -0.0051,  0.0652,  0.0189,  0.0803,  0.0740,\n",
       "            0.0359, -0.0861, -0.0300, -0.1098],\n",
       "          [-0.0663,  0.0268, -0.0017,  0.0668,  0.0183,  0.0890,  0.0817,\n",
       "            0.0424, -0.0923, -0.0268, -0.1195]],\n",
       " \n",
       "         [[-0.0239,  0.0242, -0.0081,  0.0406,  0.0170,  0.0417,  0.0389,\n",
       "            0.0124, -0.0506, -0.0275, -0.0616],\n",
       "          [-0.0438,  0.0301, -0.0079,  0.0586,  0.0195,  0.0659,  0.0610,\n",
       "            0.0259, -0.0742, -0.0319, -0.0928],\n",
       "          [-0.0575,  0.0293, -0.0051,  0.0652,  0.0189,  0.0803,  0.0740,\n",
       "            0.0359, -0.0861, -0.0300, -0.1098],\n",
       "          [-0.0663,  0.0268, -0.0017,  0.0668,  0.0183,  0.0890,  0.0817,\n",
       "            0.0424, -0.0923, -0.0268, -0.1195]]]),\n",
       " (tensor([[[-0.0663,  0.0268, -0.0017,  0.0668,  0.0183,  0.0890,  0.0817,\n",
       "             0.0424, -0.0923, -0.0268, -0.1195],\n",
       "           [-0.0663,  0.0268, -0.0017,  0.0668,  0.0183,  0.0890,  0.0817,\n",
       "             0.0424, -0.0923, -0.0268, -0.1195],\n",
       "           [-0.0663,  0.0268, -0.0017,  0.0668,  0.0183,  0.0890,  0.0817,\n",
       "             0.0424, -0.0923, -0.0268, -0.1195]]]),\n",
       "  tensor([[[-0.1489,  0.0510, -0.0034,  0.1106,  0.0369,  0.1711,  0.1437,\n",
       "             0.0884, -0.2343, -0.0583, -0.2024],\n",
       "           [-0.1489,  0.0510, -0.0034,  0.1106,  0.0369,  0.1711,  0.1437,\n",
       "             0.0884, -0.2343, -0.0583, -0.2024],\n",
       "           [-0.1489,  0.0510, -0.0034,  0.1106,  0.0369,  0.1711,  0.1437,\n",
       "             0.0884, -0.2343, -0.0583, -0.2024]]])))"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(b == 1).any(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True],\n",
       "       [False, False]])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b == 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptorch]",
   "language": "python",
   "name": "conda-env-ptorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
